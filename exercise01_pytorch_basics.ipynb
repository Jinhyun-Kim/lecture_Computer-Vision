{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "tensor = torch.tensor(data)\n",
    "print(type(tensor))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "tensor_np = torch.from_numpy(np_array)\n",
    "print(tensor_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: [[2 3]\n",
      " [4 5]]\n",
      "tensor: tensor([[2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "np.add(np_array, 1, out=np_array)\n",
    "print(f\"numpy: {np_array}\")\n",
    "print(f\"tensor: {tensor_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.2529, 0.5166, 0.1033],\n",
      "        [0.2668, 0.9063, 0.8086]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### operation locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    \n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape of tensor: torch.Size([4, 3])\n",
      "First row: tensor([1, 2, 3])\n",
      "First column: tensor([ 1,  4,  7, 10])\n",
      "Last column: tensor([ 3,  6,  9, 12])\n",
      "tensor([[ 1,  0,  3],\n",
      "        [ 4,  0,  6],\n",
      "        [ 7,  0,  9],\n",
      "        [10,  0, 12]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "print(f\"{tensor}\")\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [-0.6588,  2.8546,  1.0704,  1.6985],\n",
      "         [ 0.4942,  0.9827, -0.6635, -0.6075]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x_t = torch.ones(2,3,4)\n",
    "sub_tensor = torch.randn(2,4)\n",
    "x_t[0,1:3 ,:] = sub_tensor\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.9485, -0.6543,  1.6534,  0.0303],\n",
      "         [ 0.9485, -0.6543,  1.6534,  0.0303]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x_t[0,1:3,:] = 1\n",
    "print(x_t)\n",
    "sub_tensor = torch.randn(1,4)\n",
    "x_t[0, 1:3 ,:] = sub_tensor # broad casting\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  1,  2,  3,  1,  2,  3],\n",
      "        [ 4,  5,  6,  4,  5,  6,  4,  5,  6],\n",
      "        [ 7,  8,  9,  7,  8,  9,  7,  8,  9],\n",
      "        [10, 11, 12, 10, 11, 12, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 20., 30.])\n"
     ]
    }
   ],
   "source": [
    "x_t = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "c = 10\n",
    "\n",
    "x_t = x_t * c #broadcasting\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl_t = torch.ones((1,2))\n",
    "x2_t = torch.ones((1,2))\n",
    "xl_t + x2_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9., 12., 15.],\n",
      "        [19., 26., 33.]])\n",
      "tensor([[ 9., 12., 15.],\n",
      "        [19., 26., 33.]])\n",
      "tensor([[ 9., 12., 15.],\n",
      "        [19., 26., 33.]])\n",
      "tensor([[ 9., 12., 15.],\n",
      "        [19., 26., 33.]])\n"
     ]
    }
   ],
   "source": [
    "x1_t = torch.tensor([[1,2], [3,4]], dtype = torch.float32)\n",
    "x2_t = torch.tensor([[1,2,3],[4,5,6]], dtype = torch.float32)\n",
    "\n",
    "y1 = torch.matmul(x1_t, x2_t) # Returns tensor([[9,12,15],[19,26,33]])\n",
    "y2 = x1_t.matmul(x2_t)\n",
    "y3 = x1_t @ x2_t\n",
    "\n",
    "y4 = torch.rand_like(y1)\n",
    "torch.matmul(x1_t, x2_t, out=y4)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2], [3,4]], dtype = torch.float32)\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.]) 4.0\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum(axis = 0)\n",
    "agg_item = agg[0].item()\n",
    "print(agg, agg_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiations (autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor shape : (5)\n",
    "y = torch.zeros(3)  # output shape : (3)\n",
    "w = torch.randn(5, 3, requires_grad=True) # Weight matrix with shape (5,3)\n",
    "b = torch.randn(3, requires_grad=True) # Bias with shape (3)\n",
    "z = torch.matmul(x, w) + b\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "\n",
    "\n",
    "Forward pass에서 autograd는 아래 두가지 일을 수행:\n",
    "* run the requested operation to compute a resulting tensor\n",
    "* maintain the operation’s gradient function in the DAG (directed acyclic graph).\n",
    "\n",
    "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
    "* computes the gradients from each .grad_fn,\n",
    "* accumulates them in the respective tensor’s .grad attribute\n",
    "* using the chain rule, propagates all the way to the leaf tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7929,  0.0471,  1.6694], grad_fn=<AddBackward0>)\n",
      "Gradient function for x = None\n",
      "Gradient function for w = None\n",
      "Gradient function for z = <AddBackward0 object at 0x7f5e86e50fd0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f5e86e512d0>\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print(f\"Gradient function for x = {x.grad_fn}\")\n",
    "print(f\"Gradient function for w = {w.grad_fn}\")\n",
    "print(f\"Gradient function for z = {z.grad_fn}\") # this function knwo how to compute in backward direction\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0192, 0.1706, 0.2805],\n",
      "        [0.0192, 0.1706, 0.2805],\n",
      "        [0.0192, 0.1706, 0.2805],\n",
      "        [0.0192, 0.1706, 0.2805],\n",
      "        [0.0192, 0.1706, 0.2805]])\n",
      "tensor([0.0192, 0.1706, 0.2805])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads:  tensor(4.) tensor(6.) tensor(3.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m f\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrads: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mgrad, y\u001b[38;5;241m.\u001b[39mgrad, z\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#This cause error. gradient calculations using backward can be done once once for a given graph\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2.3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2.3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2.3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = torch.tensor(1.5, requires_grad=True)\n",
    "f = x**2+y**2+z**2\n",
    "f.backward()\n",
    "print(f\"grads: \", x.grad, y.grad, z.grad)\n",
    "f.backward() #This cause error. gradient calculations using backward can be done once once for a given graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads on First call:  tensor(4.) tensor(6.) tensor(3.)\n",
      "grads on Second call:  tensor(8.) tensor(12.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = torch.tensor(1.5, requires_grad=True)\n",
    "f = x**2+y**2+z**2\n",
    "\n",
    "f.backward(retain_graph=True)\n",
    "print(f\"grads on First call: \", x.grad, y.grad, z.grad)\n",
    "f.backward(retain_graph=True)\n",
    "print(f\"grads on Second call: \", x.grad, y.grad, z.grad) # PyTorch accumulates the gradients (실제학습에서는 optimizer를 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
